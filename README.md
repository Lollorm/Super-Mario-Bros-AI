# Super Mario Bros AI

---

# CURRENTLY WIP

![mario1-1](https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/First%20agent%20that%20managed%20to%20beat%201-1/mario1-1.gif)

An AI agent trained with the NeuroEvolution of Augmenting Topologies (NEAT) algorithm successfully beating Level 1-1  
(Even going as far as learning a frame/pixel perfect glitch called 'wall jump')

![Best fitness over generations](https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/First%20agent%20that%20managed%20to%20beat%201-1/fitness_progress_0_to_350.png)

Best fitness over generations of the previous agent (The best individual of generation 352 managed to beat the level).

---

## NEAT implementation

### About NEAT

NEAT (NeuroEvolution of Augmenting Topologies) is an evolutionary algorithm used to evolve artificial neural networks. It gradually optimizes both network structure and weights through evolutionary processes. A full description of the algorithm is available in the original [paper](https://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf).  
In this project, NEAT is implemented using the NEAT-Python library. The official documentation can be found [here](https://neat-python.readthedocs.io/en/latest/).

---

### Environment

The Gym Super Mario Bros environment was used to provide the game interface and training environment for the agent. Documentation can be found [here](https://pypi.org/project/gym-super-mario-bros/).

```
pip install gym-super-mario-bros
```

#### To play manually:

```
gym_super_mario_bros -e 'SuperMarioBrosRandomStages-v0' -m 'human' --stages '1-4'
```

#### Requirements

```
pip install gym==0.25.1 gym-super-mario-bros==7.4.0 nes-py==8.2.1 neat-python==1.1.0 numpy==1.26.4 opencv-python==4.12.0.88
```

---

### Defining an input

To select the optimal action, the neural network is fed the current environment observation at each timestep. In this implementation, the agent captures a screenshot of the current frame, converts it to grayscale, normalizes pixel values to the range [0,1], and reduces the image to a fixed-size representation of 1024 inputs. The network is configured as recurrent, so temporal information is handled through the network’s internal state rather than explicit frame stacking. As a result, no frame stacking is used. In this particular case, experiments with frame stacking using a similar number of inputs proved ineffective.

![screenshot](https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/assets/images/statescreenshot_resized.png)
![screenshot](https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/assets/images/statescreenshot.png)

The input image and its magnified version side by side.

---

### Defining a Fitness Function

When working with genetic algorithms, it all really comes down to defining a fitness function that accurately evaluates how well individuals perform a given task.  
In particular, if we want to train an AI agent to, let's say, clear level 1-1 of Super Mario Bros, we should analyze what it really means to be a good player in Super Mario.

![mario_jump](https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/assets/images/mario_jump.gif)

Here's an example of what a poorly defined fitness function can lead to (in this case jumping was rewarded too much).

In this implementation, the fitness function evaluates Mario’s speed and position along the x-axis. Penalties are applied if Mario gets stuck or dies. In addition, a small reward is given for jumping, to encourage the evolution of individuals who jump obstacles and advance further.

---

### Fitness Function
Let $t = 1, \dots, T$ denote timesteps until termination. The total fitness $F$ of an individual is defined as:
```math
F = \max \Bigg(
0.1,\;
\sum_{t=1}^{T} \Big( r_t + 0.01 + 0.1 \,(x_t - x_{t-1}) + 0.1 \,\mathbf{}_{y_t > y_{t-1}} \Big)
+ \max_t(x_t)
+ R_{\text{term}}
\Bigg)
```

where the terminal reward $R_{\text{term}}$ is:
```math
R_{\text{term}} =
\begin{cases}
+10000 & \text{if the flag is reached} \\
-150 & \text{if the agent dies before reaching the flag} \\
-100 & \text{if the agent gets stuck}
\end{cases}
```

- $r_t$ is the environment reward from *gym\_super\_mario\_bros*  
- $x_t, y_t$ are Mario's horizontal and vertical positions at timestep $t$  
- $\mathbf{}{y_t > y_{t-1}}$ is an indicator function of whether Mario is jumping or not  
- "stuck" is defined as no forward progress for more than 250 consecutive timesteps
- such a huge reward for reaching the flagpole is useful to understand in which generation the agent managed to beat the level

---

### Try it Yourself

The code to train the agent can be found in the [`NEAT`](NEAT%20Neuroevolution%20of%20Augmenting%20Topologies/First%20agent%20that%20managed%20to%20beat%201-1/) folder in this repository, it can be easily modified to change the behaviour of the agent, for example you can penalize slow agents to encourage the evolution of faster specimens that can reach speeds on par with glitchless world records.

---

## DDQN implementation

### WIP

<img src="https://github.com/Lollorm/Super-Mario-Bros-AI/blob/main/DDQN/assets/images/cartpole_qlearning-episode-0.gif" width="35%" height="35%"/>

An agent I made in the past balancing an inverted pendulum (using a Q-table).

---

### About DDQN

[paper](https://arxiv.org/pdf/1509.06461)

---

## PPO implementation

### About PPO

Proximal policy optimization (PPO) is a reinforcement learning algorithm developed by researchers at OpenAI, specifically, it is a policy gradient method.
It has proven effective in challenging environments and therefore it is a good algorithm for videogames.
Unlike off-policy algorithms such as DDQN, PPO does not use a replay buffer. As an on-policy method, PPO continuously updates its policy using data generated by the current policy only.

This time, I implemented it using stable-baselines3 [library](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) and following ClarityCoders's [tutorial](https://youtu.be/PxoG0A2QoFs?si=lWY_leDz14ngZk5S) on PPO.

The original paper can be found [here](https://arxiv.org/pdf/1707.06347v2).

---

### Environment

This time I used OpenAI's [Gym Retro](https://openai.com/index/gym-retro/) environment because it supports custom levels and allows direct access to RAM values, making it possible to inspect memory locations during gameplay.  
This time I decided to train the agent on [Super Mario World](https://en.wikipedia.org/wiki/Super_Mario_World) as it offers a more complex action space, in particular I was curious about the possibility of an agent learning trickjumps or glitches (Maybe an Agent could learn Arbitrary code execution given the proper environment).  
To do this I had to manually [integrate](https://retro.readthedocs.io/en/latest/integration.html) the game in Gym Retro, find the memory addresses of the variables that were needed and implement a reward function. By adding the necessary JSON files the game can be used as a gym environment. 




